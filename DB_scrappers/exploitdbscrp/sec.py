from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time

# Setup headless Chrome
options = Options()
options.add_argument("--headless")
options.add_argument("--disable-gpu")
driver = webdriver.Chrome(options=options)

with open("ghdb_dorks_only.txt", "w", encoding="utf-8") as file:
    for page in range(1, 531):  # 531 = All ~7944 dorks
        url = f"https://www.exploit-db.com/google-hacking-database?page={page}"
        print(f"Scraping page {page}...")
        driver.get(url)
        time.sleep(2)

        soup = BeautifulSoup(driver.page_source, "html.parser")
        rows = soup.select("table tbody tr")
        
        for row in rows:
            cols = row.find_all("td")
            if len(cols) >= 2:
                link_tag = cols[1].find("a")
                if link_tag:
                    dork = link_tag.text.strip()
                    # Filter out junk entries
                    if dork.lower() not in [
                        "search exploit-db", "submit entry", "searchsploit manual", 
                        "proving grounds", "exploit statistics"
                    ]:
                        file.write(dork + "\n")

driver.quit()
