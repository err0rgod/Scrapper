from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import os

# Setup headless Chrome for Linux (GCP-friendly)
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")  # Important for some VMs
chrome_options.add_argument("--disable-dev-shm-usage")  # Avoid /dev/shm crash

# If using chromium-browser
driver_path = "/usr/bin/chromium-browser"

driver = webdriver.Chrome(executable_path=driver_path, options=chrome_options)

with open("ghdb_dorks_only.txt", "w", encoding="utf-8") as file:
    for page in range(1, 531):  # 1 to 530 = Full ~7944 entries
        url = f"https://www.exploit-db.com/google-hacking-database?page={page}"
        print(f"[+] Scraping page {page}")
        driver.get(url)
        time.sleep(2)

        soup = BeautifulSoup(driver.page_source, "html.parser")
        rows = soup.select("table tbody tr")

        for row in rows:
            cols = row.find_all("td")
            if len(cols) >= 2:
                link_tag = cols[1].find("a")
                if link_tag:
                    dork = link_tag.text.strip()
                    # Filter out unwanted non-dork entries
                    if dork.lower() not in [
                        "search exploit-db", "submit entry", "searchsploit manual",
                        "proving grounds", "exploit statistics"
                    ]:
                        file.write(dork + "\n")

driver.quit()
print("[+] Scraping complete. Dorks saved to ghdb_dorks_only.txt")
