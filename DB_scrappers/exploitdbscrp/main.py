import requests
from bs4 import BeautifulSoup
import time

base_url = "https://www.exploit-db.com/google-hacking-database?page="
headers = {"User-Agent": "Mozilla/5.0"}

with open("ghdb_dorks_only.txt", "w", encoding="utf-8") as file:
    for page in range(1, 531):  # Adjust as needed
        print(f"Scraping page {page}...")
        url = base_url + str(page)
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.content, "html.parser")

        # Find the correct table
        table = soup.find("table", class_="exploits-table")
        if not table:
            print("No table found on page", page)
            continue

        # Find all rows
        rows = table.find("tbody").find_all("tr")

        for row in rows:
            cols = row.find_all("td")
            if len(cols) >= 2:
                # Second column contains the dork inside <a>
                dork_tag = cols[1].find("a")
                if dork_tag:
                    dork = dork_tag.get_text(strip=True)
                    file.write(dork + "\n")

        time.sleep(2)
