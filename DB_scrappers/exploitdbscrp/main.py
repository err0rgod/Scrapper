import requests
from bs4 import BeautifulSoup
import time

base_url = "https://www.exploit-db.com/google-hacking-database?page="
headers = {
    "User-Agent": "Mozilla/5.0"
}

with open("ghdb_dorks_only.txt", "w", encoding="utf-8") as file:
    for page in range(1, 531):  # Total pages currently ~530
        print(f"Scraping page {page}...")
        response = requests.get(base_url + str(page), headers=headers)
        soup = BeautifulSoup(response.content, "html.parser")

        # Target the actual <td> containing the dorks
        dork_cells = soup.select("table tbody tr td:nth-child(2) a")

        for cell in dork_cells:
            dork = cell.text.strip()
            file.write(dork + "\n")

        time.sleep(2)  # Be polite to the server
